{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "746dbb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"teyler/epstein-files-20k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9167aa05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2136420\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afff653b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text\n",
      "0                                      filename,text\n",
      "1         IMAGES-005-HOUSE_OVERSIGHT_020367.txt,\"215\n",
      "2                                                   \n",
      "3  The final choice he was made to board a non-st...\n",
      "4  To remain in Hong Kong once a criminal complai...\n",
      "(2136420, 1)\n"
     ]
    }
   ],
   "source": [
    "# Convert the train split to pandas\n",
    "df = ds[\"train\"].to_pandas()\n",
    "\n",
    "# View\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15010a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>filename,text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMAGES-005-HOUSE_OVERSIGHT_020367.txt,\"215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The final choice he was made to board a non-st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To remain in Hong Kong once a criminal complai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                      filename,text\n",
       "1         IMAGES-005-HOUSE_OVERSIGHT_020367.txt,\"215\n",
       "2                                                   \n",
       "3  The final choice he was made to board a non-st...\n",
       "4  To remain in Hong Kong once a criminal complai..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b366fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To remain in Hong Kong once a criminal complaint was leveled against him would have meant'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[4][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700fad42",
   "metadata": {},
   "source": [
    "- Create Embeddings\n",
    "- Similarity Explanation \n",
    "- Find most similar sentences \n",
    "- Create Knowledge Graph  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f062555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2136420, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22c7b764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"main_df.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23f3449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark with 10000 Rows \n",
    "df_sample = df.iloc[:10000,:]\n",
    "df_sample.to_csv(\"sample_df_test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "539bf042",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = df.iloc[:1000,:]\n",
    "df_sample.to_csv(\"sample_df_test_1000.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb52d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e930666",
   "metadata": {},
   "source": [
    "### Optimized Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21df3775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- CONFIG ----\n",
    "EMBED_MODEL = \"qwen3-embedding\"\n",
    "LLM_MODEL = \"llama3\"\n",
    "TEXT_COL = \"text\"\n",
    "MAX_WORKERS = 4  # Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94846689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a616fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = pd.read_csv(\"sample_df_test_1000.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7807fd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>filename,text</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMAGES-005-HOUSE_OVERSIGHT_020367.txt,\"215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The final choice he was made to board a non-st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>To remain in Hong Kong once a criminal complai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                                      filename,text\n",
       "1         IMAGES-005-HOUSE_OVERSIGHT_020367.txt,\"215\n",
       "2                                                NaN\n",
       "3  The final choice he was made to board a non-st...\n",
       "4  To remain in Hong Kong once a criminal complai..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33743540",
   "metadata": {},
   "source": [
    "**V1 - Not Optimized**\n",
    "1. Both models on GPU \n",
    "2. Single Pass\n",
    "3. No Chunking Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ce650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 19/1000 [01:14<1:00:36,  3.71s/it]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "import json\n",
    "import ast\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- CONFIG ----\n",
    "EMBED_MODEL = \"qwen3-embedding\"\n",
    "LLM_MODEL = \"llama3\"\n",
    "TEXT_COL = \"text\"\n",
    "MAX_WORKERS = 4  # Start with 4; adjust based on your GPU/CPU capabilities\n",
    "\n",
    "# ---- FUNCTION: keyword extraction using LLM ----\n",
    "def extract_keywords(text):\n",
    "    prompt = f\"\"\"\n",
    "    Extract concise, meaningful metadata keywords from the text below.\n",
    "    Focus on entities, topics, domain terms.\n",
    "    Return only a Python list.\n",
    "\n",
    "    TEXT:\n",
    "    {text}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        # ast.literal_eval is much safer than standard eval()\n",
    "        keywords = ast.literal_eval(response[\"message\"][\"content\"])\n",
    "    except Exception:\n",
    "        keywords = []\n",
    "\n",
    "    return keywords\n",
    "\n",
    "# ---- FUNCTION: embedding ----\n",
    "def get_embedding(text):\n",
    "    try:\n",
    "        response = ollama.embeddings(\n",
    "            model=EMBED_MODEL,\n",
    "            prompt=text\n",
    "        )\n",
    "        return response[\"embedding\"]\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# ---- FUNCTION: Worker for a single row ----\n",
    "def process_row(row_data):\n",
    "    \"\"\"Processes a single row to be used by the thread pool.\"\"\"\n",
    "    idx, text = row_data\n",
    "    text = str(text)\n",
    "\n",
    "    keywords = extract_keywords(text)\n",
    "    embedding = get_embedding(text)\n",
    "\n",
    "    return {\n",
    "        \"row_id\": int(idx),\n",
    "        \"metadata_keywords\": keywords,\n",
    "        \"embedding\": embedding\n",
    "    }\n",
    "\n",
    "# ---- MAIN PIPELINE ----\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    records = []\n",
    "        \n",
    "    # 1. Prepare data as a list of tuples (index, text)\n",
    "    row_data_list = [(idx, row[TEXT_COL]) for idx, row in sample_df.iterrows()]\n",
    "\n",
    "    # 2. Spin up the ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all tasks to the pool\n",
    "        futures = {executor.submit(process_row, row): row for row in row_data_list}\n",
    "        \n",
    "        # 3. Use as_completed to update tqdm the moment a row finishes\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures)):\n",
    "            records.append(future.result())\n",
    "\n",
    "    # 4. Optional: Sort records by row_id since threads finish out of order\n",
    "    records = sorted(records, key=lambda x: x[\"row_id\"])\n",
    "\n",
    "    # ---- SAVE TO JSON ----\n",
    "    with open(\"processed_records.json\", \"w\") as f:\n",
    "        json.dump(records, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1feb2",
   "metadata": {},
   "source": [
    "**Time Calculation**\n",
    "- 3.7 Second per 4 row ~ Almost 1 second per row \n",
    "- 21 million seconds --> TOO MUCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3149bd13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af3e0e50",
   "metadata": {},
   "source": [
    "**V2 - Optimized**\n",
    "1. Chunking \n",
    "2. 2 Pass Strategy \n",
    "3. Using SLM LLAMA 1B instead of LLAMA 8b\n",
    "4. sqllite3 usage to save processed records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d293fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pc\\AppData\\Roaming\\Python\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CPU-Optimized KeyBERT model...\n",
      "Starting Pass 1: CPU Keyword Extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 214it [3:17:17, 55.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 1 Complete! You can now run Pass 2 using the GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "import sqlite3\n",
    "import ast\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from keybert import KeyBERT\n",
    "\n",
    "# ---- CONFIG ----\n",
    "LLM_MODEL = \"llama3.2:1b\"  # Downsized for speed and VRAM efficiency\n",
    "TEXT_COL = \"text\"\n",
    "DB_PATH = \"processed_records.db\"\n",
    "CSV_PATH = \"main_df.csv\"\n",
    "CHUNK_SIZE = 512\n",
    "MAX_WORKERS = 8\n",
    "\n",
    "TOP_N_KEYWORDS = 5 # Number of keywords to extract per row\n",
    "P1_COMPUTE = \"CPU\"\n",
    "\n",
    "\n",
    "def setup_database():\n",
    "    \"\"\"Creates the SQLite table if it doesn't exist.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    # Storing keywords as a string representation of a list to keep the schema simple\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS records (\n",
    "            row_id INTEGER PRIMARY KEY,\n",
    "            text TEXT,\n",
    "            metadata_keywords TEXT,\n",
    "            status TEXT\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def process_chunk_keybert(chunk, kw_model):\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Fault Tolerance: Check which row_ids in this chunk are already in the DB\n",
    "    cursor.execute(\"SELECT row_id FROM records\")\n",
    "    existing_ids = set([row[0] for row in cursor.fetchall()])\n",
    "    \n",
    "    # Filter out rows we've already processed\n",
    "    valid_rows = [(idx, str(row['text'])) for idx, row in chunk.iterrows() if idx not in existing_ids]\n",
    "    \n",
    "    if not valid_rows:\n",
    "        conn.close()\n",
    "        return\n",
    "        \n",
    "    row_ids = [row[0] for row in valid_rows]\n",
    "    texts = [row[1] for row in valid_rows]\n",
    "    \n",
    "    # ---- THE BATCH INFERENCE ----\n",
    "    # Passing the entire list of texts to KeyBERT at once is exponentially faster on CPU\n",
    "    batch_results = kw_model.extract_keywords(\n",
    "        texts, \n",
    "        keyphrase_ngram_range=(1, 2), # Extracts single words and 2-word phrases\n",
    "        stop_words='english', \n",
    "        top_n=TOP_N_KEYWORDS,\n",
    "        use_mmr =True,\n",
    "        diversity = 0.2\n",
    "    )\n",
    "    \n",
    "    records = []\n",
    "    for idx, text_str, doc_kws in zip(row_ids, texts, batch_results):\n",
    "        # KeyBERT returns a list of tuples: [('keyword', 0.85), ('other phrase', 0.71)]\n",
    "        # We just want the words, so we strip out the confidence scores\n",
    "        words_only = [kw_tuple[0] for kw_tuple in doc_kws]\n",
    "        \n",
    "        # Save as stringified list to match our SQLite schema\n",
    "        records.append((idx, text_str, str(words_only), \"PASS_1_COMPLETE\"))\n",
    "\n",
    "    # Bulk insert\n",
    "    cursor.executemany('''\n",
    "        INSERT OR REPLACE INTO records (row_id, text, metadata_keywords, status)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "    ''', records)\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def extract_keywords(text):\n",
    "    prompt = f\"Extract concise metadata keywords. Return only a Python list.\\nTEXT:\\n{text}\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return str(ast.literal_eval(response[\"message\"][\"content\"]))\n",
    "    except Exception:\n",
    "        return \"[]\"\n",
    "\n",
    "def process_row(row_data):\n",
    "    idx, text = row_data\n",
    "    text_str = str(text)\n",
    "    keywords = extract_keywords(text_str)\n",
    "    return (idx, text_str, keywords, \"PASS_1_COMPLETE\")\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Fault Tolerance: Check which row_ids in this chunk are already in the DB\n",
    "    cursor.execute(\"SELECT row_id FROM records\")\n",
    "    existing_ids = set([row[0] for row in cursor.fetchall()])\n",
    "    \n",
    "    # Only process rows we haven't seen before\n",
    "    row_data_list = [(idx, row[TEXT_COL]) for idx, row in chunk.iterrows() if idx not in existing_ids]\n",
    "    \n",
    "    if not row_data_list:\n",
    "        return # Skip if the entire chunk is already processed\n",
    "        \n",
    "    records = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {executor.submit(process_row, row): row for row in row_data_list}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            records.append(future.result())\n",
    "\n",
    "    # Bulk insert the finished chunk to SQLite safely\n",
    "    cursor.executemany('''\n",
    "        INSERT OR REPLACE INTO records (row_id, text, metadata_keywords, status)\n",
    "        VALUES (?, ?, ?, ?)\n",
    "    ''', records)\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_database()\n",
    "    \n",
    "\n",
    "    if P1_COMPUTE == 'CPU':\n",
    "        print(\"Loading CPU-Optimized KeyBERT model...\")\n",
    "        # all-MiniLM-L6-v2 is the default: it is very small and fast for CPU execution\n",
    "        cpu_kw_model = KeyBERT(model='all-MiniLM-L6-v2') \n",
    "        \n",
    "        print(\"Starting Pass 1: CPU Keyword Extraction...\")\n",
    "        CHUNK_SIZE_CPU = 10000\n",
    "        chunk_iterator = pd.read_csv(CSV_PATH, chunksize=CHUNK_SIZE_CPU)\n",
    "        \n",
    "        for chunk in tqdm(chunk_iterator, desc=\"Processing Batches\"):\n",
    "            process_chunk_keybert(chunk, cpu_kw_model)\n",
    "            \n",
    "        print(\"Pass 1 Complete! You can now run Pass 2 using the GPU.\")\n",
    "    \n",
    "\n",
    "    else:     \n",
    "        # GPU + LLM Model\n",
    "        # Read the data file iteratively to save system RAM\n",
    "        print(\"Starting Pass 1: Keyword Extraction...\")\n",
    "        chunk_iterator = pd.read_csv(CSV_PATH, chunksize=CHUNK_SIZE)\n",
    "        \n",
    "        for chunk in tqdm(chunk_iterator, desc=\"Processing Chunks\"):\n",
    "            process_chunk(chunk)\n",
    "    \n",
    "        print(\"Pass 1 Complete. Unloading LLM from VRAM...\")    \n",
    "        # Force unload model to free GPU for Pass 2\n",
    "        ollama.chat(model=LLM_MODEL, messages=[], keep_alive=0) \n",
    "        print(\"VRAM cleared. Ready for Pass 2 (Embeddings).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce21e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "**Time Calculation**\n",
    "\n",
    "Worker 4 \n",
    "Batch 100 \n",
    "\n",
    "Starting Pass 1: Keyword Extraction...\n",
    "Processing Chunks: 10it [06:41, 40.18s/it]\n",
    "Pass 1 Complete. Unloading LLM from VRAM...\n",
    "VRAM cleared. Ready for Pass 2 (Embeddings).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10224e1e",
   "metadata": {},
   "source": [
    "### PASS 2 Embedding Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb89e0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Pass 2: Bulk Embedding Generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Vectors:   4%|▍         | 89600/2130607 [13:28:05<5794:05:21, 10.22s/it] "
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import ollama\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- CONFIG ----\n",
    "DB_PATH = \"processed_records.db\"\n",
    "EMBED_MODEL = \"qwen3-embedding\"\n",
    "BATCH_SIZE = 512  # Number of rows to send to the GPU at once\n",
    "\n",
    "def setup_pass2_db():\n",
    "    \"\"\"Ensures the SQLite database is ready to accept vector data.\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Try to add the embedding column (will fail silently if it already exists)\n",
    "    try:\n",
    "        cursor.execute(\"ALTER TABLE records ADD COLUMN embedding TEXT\")\n",
    "    except sqlite3.OperationalError:\n",
    "        pass \n",
    "        \n",
    "    conn.commit()\n",
    "    return conn\n",
    "\n",
    "def run_pass2():\n",
    "    print(\"Starting Pass 2: Bulk Embedding Generation...\")\n",
    "    \n",
    "    # Optional: Pre-load the embedding model into your cleared VRAM\n",
    "    # A negative keep_alive value keeps the model loaded in memory indefinitely\n",
    "    try:\n",
    "        ollama.chat(model=EMBED_MODEL, messages=[], keep_alive=-1)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    conn = setup_pass2_db()\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Determine exactly how much work is left to do\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM records WHERE embedding IS NULL AND status = 'PASS_1_COMPLETE'\")\n",
    "    total_unprocessed = cursor.fetchone()[0]\n",
    "    \n",
    "    if total_unprocessed == 0:\n",
    "        print(\"All rows have embeddings. Pipeline finished!\")\n",
    "        conn.close()\n",
    "        return\n",
    "\n",
    "    # Process chunks using Native Batching\n",
    "    with tqdm(total=total_unprocessed, desc=\"Generating Vectors\") as pbar:\n",
    "        while True:\n",
    "            # Fetch the next batch of rows missing embeddings\n",
    "            cursor.execute('''\n",
    "                SELECT row_id, text \n",
    "                FROM records \n",
    "                WHERE embedding IS NULL AND status = 'PASS_1_COMPLETE'\n",
    "                LIMIT ?\n",
    "            ''', (BATCH_SIZE,))\n",
    "            \n",
    "            batch = cursor.fetchall()\n",
    "            if not batch:\n",
    "                break\n",
    "                \n",
    "            row_ids = [row[0] for row in batch]\n",
    "            texts = [str(row[1]) for row in batch]\n",
    "            \n",
    "            try:\n",
    "                # Pass the entire list of strings to Ollama natively\n",
    "                response = ollama.embed(model=EMBED_MODEL, input=texts)\n",
    "                embeddings = response[\"embeddings\"]\n",
    "                \n",
    "                # Pair up the new embeddings with their database row IDs\n",
    "                update_data = []\n",
    "                for idx, emb in zip(row_ids, embeddings):\n",
    "                    update_data.append((json.dumps(emb), idx))\n",
    "                    \n",
    "                # Bulk update the SQLite database\n",
    "                cursor.executemany(\"UPDATE records SET embedding = ? WHERE row_id = ?\", update_data)\n",
    "                conn.commit()\n",
    "                \n",
    "                pbar.update(len(batch))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing batch at GPU level: {e}\")\n",
    "                break\n",
    "                \n",
    "    conn.close()\n",
    "    print(\"Pass 2 Complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_pass2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef1c483",
   "metadata": {},
   "source": [
    "### SQL Lite to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6583f0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Export: SQLite to Parquet...\n",
      "Found 1000 completed records. Exporting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing Parquet: 100%|██████████| 1000/1000 [00:00<00:00, 1022.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete! Highly compressed dataset saved to final_output.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- CONFIG ----\n",
    "DB_PATH = \"processed_records.db\"\n",
    "PARQUET_PATH = \"final_output.parquet\"\n",
    "CHUNK_SIZE = 250000  # Process a quarter-million rows at a time\n",
    "\n",
    "def export_to_parquet():\n",
    "    print(\"Starting Export: SQLite to Parquet...\")\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    \n",
    "    # 1. Count total completed rows for the progress bar\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM records WHERE status = 'PASS_1_COMPLETE' AND embedding IS NOT NULL\")\n",
    "    total_rows = cursor.fetchone()[0]\n",
    "    \n",
    "    if total_rows == 0:\n",
    "        print(\"No completed records found to export.\")\n",
    "        conn.close()\n",
    "        return\n",
    "        \n",
    "    print(f\"Found {total_rows} completed records. Exporting...\")\n",
    "\n",
    "    # 2. Setup the Parquet Writer\n",
    "    writer = None\n",
    "    offset = 0\n",
    "    \n",
    "    with tqdm(total=total_rows, desc=\"Writing Parquet\") as pbar:\n",
    "        while True:\n",
    "            # 3. Read chunk from SQLite safely\n",
    "            query = f\"\"\"\n",
    "                SELECT row_id, text, metadata_keywords, embedding \n",
    "                FROM records \n",
    "                WHERE status = 'PASS_1_COMPLETE' AND embedding IS NOT NULL\n",
    "                LIMIT {CHUNK_SIZE} OFFSET {offset}\n",
    "            \"\"\"\n",
    "            chunk_df = pd.read_sql_query(query, conn)\n",
    "            \n",
    "            if chunk_df.empty:\n",
    "                break\n",
    "                \n",
    "            # 4. Clean up stringified data into real Python/Arrow lists\n",
    "            chunk_df['metadata_keywords'] = chunk_df['metadata_keywords'].apply(\n",
    "                lambda x: ast.literal_eval(x) if pd.notnull(x) else []\n",
    "            )\n",
    "            chunk_df['embedding'] = chunk_df['embedding'].apply(\n",
    "                lambda x: json.loads(x) if pd.notnull(x) else []\n",
    "            )\n",
    "            \n",
    "            # 5. Convert DataFrame to PyArrow Table\n",
    "            table = pa.Table.from_pandas(chunk_df)\n",
    "            \n",
    "            # 6. Initialize writer on the first pass (requires the table schema)\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(PARQUET_PATH, table.schema, compression='snappy')\n",
    "            \n",
    "            # 7. Append chunk directly to the file\n",
    "            writer.write_table(table)\n",
    "            \n",
    "            offset += CHUNK_SIZE\n",
    "            pbar.update(len(chunk_df))\n",
    "            \n",
    "    if writer:\n",
    "        writer.close()\n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"Export complete! Highly compressed dataset saved to {PARQUET_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    export_to_parquet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a482149",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('final_output.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a56309d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>text</th>\n",
       "      <th>metadata_keywords</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>filename,text</td>\n",
       "      <td>[filename text, filename, text]</td>\n",
       "      <td>[0.023052093, -0.028317599, 0.011062983, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>IMAGES-005-HOUSE_OVERSIGHT_020367.txt,\"215</td>\n",
       "      <td>[house_oversight_020367 txt, 005 house_oversig...</td>\n",
       "      <td>[-0.0015994079, 0.0009793631, -0.010118909, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>nan</td>\n",
       "      <td>[nan]</td>\n",
       "      <td>[0.031404983, 0.03075171, -0.02906024, -0.0345...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The final choice he was made to board a non-st...</td>\n",
       "      <td>[flight moscow, moscow june, stop flight, flig...</td>\n",
       "      <td>[0.027918369, 0.005494184, -0.0023703119, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>To remain in Hong Kong once a criminal complai...</td>\n",
       "      <td>[remain hong, kong criminal, hong kong, hong, ...</td>\n",
       "      <td>[-0.004669463, 0.009473486, -0.0022050955, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>including cross-cultural analyses (similaritie...</td>\n",
       "      <td>[differences dream, dream posting, cultural an...</td>\n",
       "      <td>[-0.00082270417, 0.017136006, 0.0028171376, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>and structure of dream data sets. This enables...</td>\n",
       "      <td>[dream data, studies dreams, structure dream, ...</td>\n",
       "      <td>[0.026132682, 0.004024183, -0.0052859643, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>Patterns are repeated. For example, in general...</td>\n",
       "      <td>[dreams males, dreams, general dreams, pattern...</td>\n",
       "      <td>[0.003786812, 0.031239565, 0.013577143, -0.028...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>Males will engage in fighting with other males...</td>\n",
       "      <td>[fighting males, male dreamers, males engage, ...</td>\n",
       "      <td>[0.008204588, 0.031083258, 0.0074874726, -0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>are often the initiator of fights and the figh...</td>\n",
       "      <td>[females fight, fights usually, fight verbally...</td>\n",
       "      <td>[-0.0063914508, 0.015579377, 0.012610367, -0.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     row_id                                               text  \\\n",
       "0         0                                      filename,text   \n",
       "1         1         IMAGES-005-HOUSE_OVERSIGHT_020367.txt,\"215   \n",
       "2         2                                                nan   \n",
       "3         3  The final choice he was made to board a non-st...   \n",
       "4         4  To remain in Hong Kong once a criminal complai...   \n",
       "..      ...                                                ...   \n",
       "995     995  including cross-cultural analyses (similaritie...   \n",
       "996     996  and structure of dream data sets. This enables...   \n",
       "997     997  Patterns are repeated. For example, in general...   \n",
       "998     998  Males will engage in fighting with other males...   \n",
       "999     999  are often the initiator of fights and the figh...   \n",
       "\n",
       "                                     metadata_keywords  \\\n",
       "0                      [filename text, filename, text]   \n",
       "1    [house_oversight_020367 txt, 005 house_oversig...   \n",
       "2                                                [nan]   \n",
       "3    [flight moscow, moscow june, stop flight, flig...   \n",
       "4    [remain hong, kong criminal, hong kong, hong, ...   \n",
       "..                                                 ...   \n",
       "995  [differences dream, dream posting, cultural an...   \n",
       "996  [dream data, studies dreams, structure dream, ...   \n",
       "997  [dreams males, dreams, general dreams, pattern...   \n",
       "998  [fighting males, male dreamers, males engage, ...   \n",
       "999  [females fight, fights usually, fight verbally...   \n",
       "\n",
       "                                             embedding  \n",
       "0    [0.023052093, -0.028317599, 0.011062983, -0.02...  \n",
       "1    [-0.0015994079, 0.0009793631, -0.010118909, -0...  \n",
       "2    [0.031404983, 0.03075171, -0.02906024, -0.0345...  \n",
       "3    [0.027918369, 0.005494184, -0.0023703119, -0.0...  \n",
       "4    [-0.004669463, 0.009473486, -0.0022050955, 0.0...  \n",
       "..                                                 ...  \n",
       "995  [-0.00082270417, 0.017136006, 0.0028171376, 0....  \n",
       "996  [0.026132682, 0.004024183, -0.0052859643, -0.0...  \n",
       "997  [0.003786812, 0.031239565, 0.013577143, -0.028...  \n",
       "998  [0.008204588, 0.031083258, 0.0074874726, -0.03...  \n",
       "999  [-0.0063914508, 0.015579377, 0.012610367, -0.0...  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df76b9cb",
   "metadata": {},
   "source": [
    "### Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c0f3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text):\n",
    "    print(text)\n",
    "    prompt = f\"\"\"Extract concise metadata keywords from the text below. \n",
    "    You must respond ONLY in strict JSON format like this: {{\"keywords\": [\"word1\", \"word2\"]}}\n",
    "    \n",
    "    TEXT:\n",
    "    {text}\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=LLM_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            format = \"json\",\n",
    "                    options={\n",
    "                \"temperature\": 0.0,  # 0.0 makes the output strictly factual and deterministic\n",
    "                \"top_p\": 0.1,        # Restricts the model to only the highest-probability words\n",
    "            }\n",
    "        )\n",
    "        return str(ast.literal_eval(response[\"message\"][\"content\"]))\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "        return \"[]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1590b85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vital part of maintaining normal brain and mental functioning. New neurons are being created, importantly, in \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"{'keywords': ['brain', 'neurons', 'mental functioning', 'new neurons']}\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keywords(\"vital part of maintaining normal brain and mental functioning. New neurons are being created, importantly, in \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7a7a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
